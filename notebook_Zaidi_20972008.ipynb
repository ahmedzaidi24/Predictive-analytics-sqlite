{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries Used"
      ],
      "metadata": {
        "id": "kyMGa1i2SEt0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing all the relevant libraries that might be needed in the assignment:**"
      ],
      "metadata": {
        "id": "a87UsT0e9YbS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWq-enDM4UFi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sqlite3\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Data"
      ],
      "metadata": {
        "id": "DAzuf6BRSLwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please Import the Assignment2023.sqlite file here to continue the code"
      ],
      "metadata": {
        "id": "Cv5Kq0YA9kF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://github.com/PaulHancock/COMP5009_pracs/raw/main/data/Assignment2023.sqlite"
      ],
      "metadata": {
        "id": "n_iLniTmGs7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con = sqlite3.connect('Assignment2023.sqlite')\n",
        "test_df = pd.read_sql_query('SELECT * FROM test;',con)\n",
        "train_df = pd.read_sql_query('SELECT * FROM train;',con)"
      ],
      "metadata": {
        "id": "__E_Pg6kCUoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To take an overall look at the whole train data if done right\n",
        "test_df\n"
      ],
      "metadata": {
        "id": "oMqYqma5EzJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "V_vXUbemSoro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To take a look at the heads of both tables with the limit of 20\n",
        "train_df.head(20)\n",
        "test_df.head(20)"
      ],
      "metadata": {
        "id": "DVu52Q1jHxae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check the shape of your data to see how many rows and columns it has\n",
        "print(\"Test dataset shape:\", test_df.shape)\n",
        "print(\"Train dataset shape:\", train_df.shape)\n"
      ],
      "metadata": {
        "id": "SGHUNDe0RYO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Print the names of the columns\n",
        "print(\"Column Names\")\n",
        "print(train_df.columns)"
      ],
      "metadata": {
        "id": "CXb88EEsRp3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To describe each data attribute in more detail\n",
        "train_df.describe()"
      ],
      "metadata": {
        "id": "ygK8FIG9UP-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "7TfLhnsfYo57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dealing with Missing Values**"
      ],
      "metadata": {
        "id": "S-J6UeB3r5x-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCzU_hqygp1"
      },
      "source": [
        "# find which columns have missing data\n",
        "def missing(df):\n",
        "\n",
        "  missing_dict = dict()\n",
        "  total = df.shape[0] # shape[0] is the number of rows\n",
        "  for attribute in df.columns:\n",
        "    missing = df[attribute].isna().sum() # count the number of Null/nan/na values\n",
        "    frac = missing/total * 100 # as a percentage\n",
        "    missing_dict[attribute] = frac\n",
        "  return missing_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_dict = missing(train_df)\n",
        "m_dict"
      ],
      "metadata": {
        "id": "NAfZrD4YY6GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To find out which attributes have missing values more than 50%\n",
        "cols_to_drop = [ att for att,frac in m_dict.items() if frac >50]\n",
        "cols_to_drop"
      ],
      "metadata": {
        "id": "Wuucv0_BjcwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD_4w27v2XZO"
      },
      "source": [
        "# As only one of them had more than 50 so we delete it from both train and test data set\n",
        "cols_to_drop = ['Att24'] # Fill this in\n",
        "train_df.drop(columns=cols_to_drop,\n",
        "           inplace=True)\n",
        "test_df.drop(columns=cols_to_drop,\n",
        "           inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm that our data frame now has fewer columns (was 280)\n",
        "train_df.columns\n",
        "test_df.columns"
      ],
      "metadata": {
        "id": "3gagSk7skrwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbZ6j8id3k9w"
      },
      "source": [
        "# Now finding columns with missing values less than 50%\n",
        "cols_to_impute = [ att for att,frac in m_dict.items() if 0<frac <50]\n",
        "cols_to_impute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Att07'].interpolate(method='linear', inplace=True)\n",
        "test_df['Att07'].interpolate(method='linear', inplace=True)\n",
        "\n",
        "train_df['Att15'].interpolate(method='linear', inplace=True)\n",
        "test_df['Att15'].interpolate(method='linear', inplace=True)"
      ],
      "metadata": {
        "id": "kayeH6QGnESB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tud90Nf7rxg3"
      },
      "source": [
        "# find which columns have missing data\n",
        "def missing(df):\n",
        "  missing_dict = dict()\n",
        "  total = df.shape[0] # shape[0] is the number of rows\n",
        "  for attribute in df.columns:\n",
        "    missing = df[attribute].isna().sum() # count the number of Null/nan/na values\n",
        "    frac = missing/total * 100 # as a percentage\n",
        "    missing_dict[attribute] = frac\n",
        "  return missing_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_dict = missing(train_df)\n",
        "m_dict"
      ],
      "metadata": {
        "id": "e-Ew2OXery-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dealing with Duplicate Columns and Rows**"
      ],
      "metadata": {
        "id": "wA1kRyZhxSV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding duplicates in columns and rows for Train data\n",
        "\n",
        "duplicate_columns = train_df.columns[train_df.columns.duplicated()]\n",
        "if len(duplicate_columns) > 0:\n",
        "    print(\"Duplicate columns found:\")\n",
        "    print(duplicate_columns)\n",
        "else:\n",
        "    print(\"No duplicate columns found.\")\n",
        "\n",
        "\n",
        "dups = train_df.duplicated()\n",
        "dups.sum()"
      ],
      "metadata": {
        "id": "OX5IlSnLx15r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNlxj3MlCr5J"
      },
      "source": [
        "dups = train_df.duplicated()\n",
        "dups.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding duplicates in columns and rows for Test data\n",
        "\n",
        "duplicate_columns = test_df.columns[test_df.columns.duplicated()]\n",
        "if len(duplicate_columns) > 0:\n",
        "    print(\"Duplicate columns found:\")\n",
        "    print(duplicate_columns)\n",
        "else:\n",
        "    print(\"No duplicate columns found.\")\n",
        "\n",
        "dups = test_df.duplicated()\n",
        "dups.sum()"
      ],
      "metadata": {
        "id": "F9at8NhVxcXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Types and Binarization**\n",
        "\n"
      ],
      "metadata": {
        "id": "0aa-Cjq5G5Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To find the data types of each column\n",
        "print(\"Data types\")\n",
        "print(train_df.dtypes)"
      ],
      "metadata": {
        "id": "ITRfwFCSUE4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To find out which columns are categorical\n",
        "column_data_types = train_df.dtypes\n",
        "categorical_columns = column_data_types[column_data_types == 'object'].index\n",
        "print(\"Categorical Columns:\", categorical_columns.tolist())"
      ],
      "metadata": {
        "id": "redkMx6dHN1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To find the the number of unique categories in the above mentioned columns\n",
        "unique_categories_col1 = train_df['Att11'].nunique()\n",
        "unique_categories_col2 = train_df['Att16'].nunique()\n",
        "unique_categories_col3 = train_df['Att25'].nunique()\n",
        "\n",
        "print(\"Unique categories in 'categorical_column1':\", unique_categories_col1)\n",
        "print(\"Unique categories in 'categorical_column2':\", unique_categories_col2)\n",
        "print(\"Unique categories in 'categorical_column3':\", unique_categories_col3)\n"
      ],
      "metadata": {
        "id": "rDD7NkQkKe89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To find the frequency of occurance of each category in the dataset\n",
        "categorical_columns = ['Att11', 'Att16', 'Att25']\n",
        "\n",
        "for col_name in categorical_columns:\n",
        "    unique_categories_counts = train_df[col_name].value_counts()\n",
        "\n",
        "    print(f\"Unique categories in '{col_name}':\")\n",
        "    for category, count in unique_categories_counts.iteritems():\n",
        "        print(f\"{category}: {count}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "AFB-dJwaLES-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting each into Numerical (Binarization) for train and test\n",
        "\n",
        "new_Att11= pd.get_dummies(train_df.Att11,prefix=\"Att11\")\n",
        "new_Att16= pd.get_dummies(train_df.Att16,prefix=\"Att16\")\n",
        "new_Att25= pd.get_dummies(train_df.Att25,prefix=\"Att25\")\n",
        "\n",
        "\n",
        "new_Att11= pd.get_dummies(test_df.Att11,prefix=\"Att11\")\n",
        "new_Att16= pd.get_dummies(test_df.Att16,prefix=\"Att16\")\n",
        "new_Att25= pd.get_dummies(test_df.Att25,prefix=\"Att25\")"
      ],
      "metadata": {
        "id": "Dtn_2WrhLu7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge Att11, Att16 and Att25 for training dataset\n",
        "train_df_2 = pd.concat((train_df, new_Att11,new_Att16,new_Att25), axis=1)\n",
        "test_df_2 = pd.concat((test_df, new_Att11,new_Att16,new_Att25), axis=1)"
      ],
      "metadata": {
        "id": "S6yTwjwdV7JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping all the previous colunmns\n",
        "train_df_2.drop(columns = 'Att11', inplace=True)\n",
        "train_df_2.drop(columns = 'Att16', inplace=True)\n",
        "train_df_2.drop(columns = 'Att25', inplace=True)\n",
        "\n",
        "test_df_2.drop(columns = 'Att11', inplace=True)\n",
        "test_df_2.drop(columns = 'Att16', inplace=True)\n",
        "test_df_2.drop(columns = 'Att25', inplace=True)\n"
      ],
      "metadata": {
        "id": "IsBfEUUXW7zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the column index (position) of 'class'\n",
        "class_column_index = train_df_2.columns.get_loc('class')\n",
        "\n",
        "# Print the column index\n",
        "print(\"Column Index of 'class':\", class_column_index)\n"
      ],
      "metadata": {
        "id": "gYpKrqJ7cGp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change order of the class to last column in train and test data\n",
        "titles = list(train_df_2.columns)\n",
        "titles[27],titles[-1] = titles[-1],titles[27]\n",
        "train_df_2 = train_df_2[titles]\n",
        "\n",
        "titles_test = list(test_df_2.columns)\n",
        "titles_test[27],titles_test[-1] = titles_test[-1],titles_test[27]\n",
        "test_df_2 = test_df_2[titles_test]"
      ],
      "metadata": {
        "id": "ZjxYnbOIbwbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df_2.columns)\n",
        "train_df_2.head(40)"
      ],
      "metadata": {
        "id": "yFdT9AlVcSp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "modwS7YbRURO"
      },
      "outputs": [],
      "source": [
        "#drop class attribute in the test_df_2 as that is what we need to predict by 0,1,2\n",
        "test_df_2.drop(columns = 'class', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # It's necessary to store the \"index\" attribute as a variable for future arrangements.\n",
        "ind = test_df['index']"
      ],
      "metadata": {
        "id": "UrkWAgCi16P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding Correlation between Attributes**"
      ],
      "metadata": {
        "id": "IS-veOd6_tVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cor = train_df_2.corr(method='pearson')\n",
        "cor.style.background_gradient(cmap='coolwarm').set_precision(2)"
      ],
      "metadata": {
        "id": "QBoz3yFE-T2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now deleting all attributes that are having correlation of more than 80%\n",
        "\n",
        "train_df_2.drop(columns = 'Att00', inplace=True)\n",
        "train_df_2.drop(columns = 'Att03', inplace=True)\n",
        "train_df_2.drop(columns = 'Att06', inplace=True)\n",
        "train_df_2.drop(columns = 'Att09', inplace=True)\n",
        "train_df_2.drop(columns = 'Att13', inplace=True)\n",
        "train_df_2.drop(columns = 'Att25_ASCZ', inplace=True)\n",
        "\n",
        "\n",
        "test_df_2.drop(columns = 'Att00', inplace=True)\n",
        "test_df_2.drop(columns = 'Att03', inplace=True)\n",
        "test_df_2.drop(columns = 'Att06', inplace=True)\n",
        "test_df_2.drop(columns = 'Att09', inplace=True)\n",
        "test_df_2.drop(columns = 'Att13', inplace=True)\n",
        "test_df_2.drop(columns = 'Att25_ASCZ', inplace=True)"
      ],
      "metadata": {
        "id": "oZgxaFdB-joc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI-45ahWIlso"
      },
      "outputs": [],
      "source": [
        "#KNn\n",
        "X = train_df_2.iloc[:,:-1] #storing without the class attribute\n",
        "y = train_df_2.iloc[:, -1] # y contains the class attribute"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = test_df_2.iloc[:,:] #storing without class attribute in the test attribute"
      ],
      "metadata": {
        "id": "Y2KXe8473CS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "8ddbAfoqqwCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "ml0zuAP-ru8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "9nPtigJQrmgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SCALING WITH Z SCORE**"
      ],
      "metadata": {
        "id": "Sfxnw7KHUBMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # List of column names to convert to integer (replace with your actual column names)\n",
        "columns_to_convert = ['Att11_AQDH', 'Att11_LCAS', 'Att11_NAAU', 'Att11_NBAE', 'Att11_QQNT',\n",
        "                      'Att11_USGL', 'Att16_FBLE', 'Att16_ITRV', 'Att16_MFLQ', 'Att16_PWEH',\n",
        "                      'Att16_QKNH', 'Att16_VVQP', 'Att16_XCYU', 'Att16_YCQC', 'Att16_YNCP',\n",
        "                      'Att16_ZFBS', 'Att25_CDJW']\n",
        "\n",
        "# Convert the selected columns to integer for train\n",
        "X[columns_to_convert] = X[columns_to_convert].fillna(0).astype(int)\n",
        "X[columns_to_convert] = X[columns_to_convert].fillna(0).astype(int)\n"
      ],
      "metadata": {
        "id": "YMBPqeKdRi3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of column names to convert to integer (replace with your actual column names)\n",
        "columns_to_convert = ['Att11_AQDH', 'Att11_LCAS', 'Att11_NAAU', 'Att11_NBAE', 'Att11_QQNT',\n",
        "                      'Att11_USGL', 'Att16_FBLE', 'Att16_ITRV', 'Att16_MFLQ', 'Att16_PWEH',\n",
        "                      'Att16_QKNH', 'Att16_VVQP', 'Att16_XCYU', 'Att16_YCQC', 'Att16_YNCP',\n",
        "                      'Att16_ZFBS', 'Att25_CDJW']\n",
        "\n",
        "# Convert the selected columns to integer for test data\n",
        "X_test[columns_to_convert] = X_test[columns_to_convert].fillna(0).astype(int)\n",
        "X_test[columns_to_convert] = X_test[columns_to_convert].fillna(0).astype(int)\n"
      ],
      "metadata": {
        "id": "beNwq-KI6Shx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check if they are in integer now\n",
        "print(\"Data types\")\n",
        "print(X.dtypes)"
      ],
      "metadata": {
        "id": "IoLCMO4UTWiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store float64 column names\n",
        "float_columns = []\n",
        "\n",
        "# Iterate through the DataFrame's columns\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'float64':\n",
        "        float_columns.append(col)\n"
      ],
      "metadata": {
        "id": "1pGS8XwfUl_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_training = scaler.fit_transform(X[float_columns])\n",
        "X_testing = scaler.fit_transform(X_test[float_columns])"
      ],
      "metadata": {
        "id": "yTO_DTOPV0hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the dataset into two subsets\n",
        "# train data is 75% and test data is 25%\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_training, y,\n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state=4)"
      ],
      "metadata": {
        "id": "Yj1UCeo39UZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mj2j-goLmRi"
      },
      "outputs": [],
      "source": [
        "#total 5000 elements divided 75% into train data and 25% into test data\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.describe()"
      ],
      "metadata": {
        "id": "KQ1ZcEBd98E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data classification"
      ],
      "metadata": {
        "id": "3e472ZYaHU40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Model**"
      ],
      "metadata": {
        "id": "iZVykG7h1rJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, ShuffleSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn import naive_bayes\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "XsIuFg9w1w1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=10)\n",
        "kf = KFold(n_splits=10)\n",
        "ss = ShuffleSplit(n_splits=10, test_size=15, random_state=4)"
      ],
      "metadata": {
        "id": "eluJv53Am3yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL IMBALANCE CHECK**"
      ],
      "metadata": {
        "id": "Fq1TI4N_uuT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_2.groupby(['class']).count()"
      ],
      "metadata": {
        "id": "TPamjYPctPOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot to show unbalanced nature of the 'class' attribute in train_df_2\n",
        "LABELS = ['2', '1', '0']\n",
        "count_classes = pd.value_counts(train_df_2['class'], sort=True)\n",
        "colors = ['lightcoral', 'lightskyblue', 'lightgreen']\n",
        "count_classes.plot(kind='pie', rot=0, autopct='%1.1f%%', colors=colors)\n",
        "plt.title('Distribution of \"Class\" attribute')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7g-ixXeGtMJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SMOTE needs to be applied to balance the 'class' attribute and then check it by doing KNN\n",
        "smote = SMOTE()\n",
        "\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train.astype(\"float\"), y_train)\n",
        "print(\"Before SMOTE: \", Counter(y_train))\n",
        "print(\"After SMOTE: \", Counter(y_train_smote))"
      ],
      "metadata": {
        "id": "gl9sgxtjtsKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN METHOD**"
      ],
      "metadata": {
        "id": "-gpAQv2yun1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of all the parameters we'll be iterating over\n",
        "parameters = {'weights': ['uniform','distance'],\n",
        "              'n_neighbors':[1,3,7,11]} #  list of the nearest neigbhours\n",
        "# make a classifier object\n",
        "knn = KNeighborsClassifier()\n",
        "# create a GridSearchCV object to do the training with cross validation\n",
        "gscv = GridSearchCV(estimator=knn,\n",
        "                    param_grid=parameters,\n",
        "                    cv=skf,  # the cross validation folding pattern\n",
        "                    scoring='accuracy')\n",
        "# now train our model\n",
        "best_knn = gscv.fit(X_train, y_train)\n",
        "best_knn.best_params_, best_knn.best_score_"
      ],
      "metadata": {
        "id": "-Xa-HlA1m6dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=11,weights='distance')\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)"
      ],
      "metadata": {
        "id": "Dzsf2DrynIB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "best_knn.fit(X_train, y_train)\n",
        "y_predict = best_knn.predict(X_test)\n",
        "print(\"accuracy:\", accuracy_score(y_test,y_predict)*(100),'%')\n",
        "pd.crosstab(y_test,y_predict)"
      ],
      "metadata": {
        "id": "l9ojzubhnLGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics._plot.confusion_matrix import confusion_matrix\n",
        "cfm = confusion_matrix(y_test, y_predict)\n",
        "cfm"
      ],
      "metadata": {
        "id": "KST7zIBVnQDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "fuvifdf3nSLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN after balancing the attribute\n",
        "best_knn.fit(X_train_smote, y_train_smote)\n",
        "y_predict = best_knn.predict(X_test)\n",
        "print(\"accuracy:\", accuracy_score(y_test,y_predict)*(100),'%')\n",
        "pd.crosstab(y_test,y_predict)"
      ],
      "metadata": {
        "id": "Txr9o2yHtygI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree Classifier**"
      ],
      "metadata": {
        "id": "NCTP5UsOwZ2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'criterion': ('gini','entropy'),  # this should be the different splitting criteria\n",
        "              'min_samples_split':[3,13,15,20]} # this should be the different values for min_samples_split\n",
        "dtc = tree.DecisionTreeClassifier()\n",
        "gscv = GridSearchCV(estimator=dtc,\n",
        "                    param_grid=parameters,\n",
        "                    cv=ss,\n",
        "                    scoring='accuracy')\n",
        "best_dtc = gscv.fit(X_train, y_train)\n",
        "best_dtc.best_params_, best_dtc.best_score_"
      ],
      "metadata": {
        "id": "xW1Nr7lIwd_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_dtc,y_train_dtc = X_train,y_train\n",
        "dtc = tree.DecisionTreeClassifier(criterion = 'entropy',min_samples_split= 13)\n",
        "dtc.fit(X_train_dtc, y_train_dtc)\n",
        "y_pred = dtc.predict(X_test)"
      ],
      "metadata": {
        "id": "6xAnmOQXxC2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\",accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "X9wVCJUmxGHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#apply decision tree model after balancing the elements\n",
        "best_dtc.fit(X_train_smote, y_train_smote)\n",
        "y_predict = best_dtc.predict(X_test)\n",
        "print(\"accuracy:\", accuracy_score(y_test,y_predict)*(100),'%')\n",
        "pd.crosstab(y_test,y_predict)"
      ],
      "metadata": {
        "id": "6iFzwrqixV4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "tM711vSqxxMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no parameters to adjust so no need to optimise, just train\n",
        "fig, ax = plt.subplots(1,1)\n",
        "# Training our decision tree model\n",
        "nb = naive_bayes.GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_predict = nb.predict(X_test)\n",
        "print(\"accuracy:\", accuracy_score(y_test,y_predict)*(100),'%')\n",
        "\n",
        "class_labels = [\"0.0\", \"1.0\", \"2.0\"]\n",
        "ConfusionMatrixDisplay.from_estimator(nb,\n",
        "                                      X_test, y_test,\n",
        "                                      display_labels=class_labels,  # Provide your class labels here\n",
        "                                      ax=ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YvH5LI9FzQLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KN-Neighbor model with the balanced 'class' attribute in test dataset\n",
        "best_knn.fit(X_train_smote, y_train_smote)\n",
        "y_predict_knn = best_knn.predict(X_testing)"
      ],
      "metadata": {
        "id": "1ieI2ULuzXgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision tree classifier model with the balanced 'class' attribute in test dataset\n",
        "best_dtc.fit(X_train_smote, y_train_smote)\n",
        "y_predict_dtc = best_dtc.predict(X_testing)"
      ],
      "metadata": {
        "id": "focX8dXz00Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_knn = pd.DataFrame(y_predict_knn)\n",
        "dataset_dtc = pd.DataFrame(y_predict_dtc)"
      ],
      "metadata": {
        "id": "dJOmJwjF02Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate our predictions with the index column\n",
        "prediction = pd.concat((ind,dataset_knn,dataset_dtc), axis=1)\n",
        "prediction.columns =['index', 'Predict1', 'Predict2']\n",
        "\n"
      ],
      "metadata": {
        "id": "dOHjaY3e04ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction['Predict1'] = prediction['Predict1'].astype(int)\n",
        "prediction['Predict2'] = prediction['Predict2'].astype(int)\n",
        "\n",
        "#display the results to check the format\n",
        "prediction"
      ],
      "metadata": {
        "id": "Su5yVfHP071D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save results in the SQL file\n",
        "save_location = '/content/Answers.sqlite'\n",
        "con = sqlite3.connect('Answers.sqlite')\n",
        "prediction.to_sql(name='Answers.sqlite', con=con)\n",
        "con.close()"
      ],
      "metadata": {
        "id": "Igbz4nmX6DFl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}